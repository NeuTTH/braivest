{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braivest.utils import load_wandb_model\n",
    "from braivest.model.emgVAE import emgVAE\n",
    "from braivest.preprocess.dataset_utils import load_data, bin_data, find_artifacts\n",
    "from braivest.analysis.plotting_utils import *\n",
    "from braivest.analysis.hmm_utils import *\n",
    "\n",
    "import plotly.express as px\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ssm.hmm import MultiHMM, HMM\n",
    "import ssm\n",
    "from pyvis.network import Network\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_data_artifact_dir = \n",
    "subject0_sess = load_data(artifact_dir, 'subject0_vis11_val.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =load_wandb_model(\"juliahwang/lfp_VAE/v2l9tltt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fitting the HMM, we want datasets that are continuous. So we need to split the data every time there is an artifact.\n",
    "subject0_sessions = [0,2,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "encodings_all = []\n",
    "for sess in range(1, len(subject0_sessions)):\n",
    "    lfp, emg = get_raw_session_data(0, subject0_sessions[sess],12)\n",
    "    artifacts = find_artifacts(lfp)\n",
    "    encodings_full = model.encode(subject0_sess[sess][0])\n",
    "    encodings_split = np.split(encodings_full, artifacts)\n",
    "    encodings_split_mod = [split[1:] for split in encodings_split if len(split) > 1]\n",
    "    encodings_all.extend(encodings_split_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First use cross validation to find the correct number of clusters\n",
    "scores = []\n",
    "scores_std = []\n",
    "for n_clusters in range(2, 15):\n",
    "    hmm, train_scores, test_scores = hmm_cross_val(n_clusters, encodings_all, n_repeats=3)\n",
    "    scores.append(np.mean(test_scores))\n",
    "    scores_std.append(np.std(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(range(2, 15),np.asarray(scores)*-1, yerr = scores_std)\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Negative Log Likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.asarray(scores)\n",
    "plt.plot(range(3,15),(scores[1:]*-1 - scores[:-1]*-1)/scores[:-1]*-1)\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Percent change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(8, 2)\n",
    "hmm_lls = hmm.fit(encodings_all, method=\"em\", num_iters=50, init_method=\"kmeans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_labels = get_hmm_labels(hmm, encodings_all[:10])\n",
    "fig = plot_encodings(np.concatenate(encodings_all[:10], axis=0), color=np.concatenate(sess_labels), range_x = (-6, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = {0:'SWS1', 1:'AE', 2: 'REM', 3: 'T1', 4: 'SWS2',5:'SWS3', 6:'Wake', 7:'T2'}\n",
    "color_map = {'REM':'#2986cc', \"Wake\":\"#e67f38\", 'AE':'#f44336', \"T1\":\"#d3f758\", 'SWS1':'#93c432','SWS2':'#789837' ,'SWS3':'#38761d', 'T2':'#ecf132'} \n",
    "fig = plot_encodings(np.concatenate(encodings_all[:10], axis=0), color=[legend[l] for l in np.concatenate(sess_labels)], color_map= color_map, range_x = (-6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_labels = np.concatenate(get_hmm_labels(hmm, encodings_all))\n",
    "colors = [color_map[legend[s]] for s in range(8)]\n",
    "inferred_durations, fig = plot_state_durations(sess_labels, 8, ordering =[0, 3, 4, 7, 5, 6, 2, 1], color_list=colors)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transition_graph(8, hmm.transitions.transition_matrix, sess_labels, colors, \"transition_graph.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8f7389857a67e13edb5c3f00a3f763f3f77988fd78555eef11cd2d761496f8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
